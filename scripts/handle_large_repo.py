#!/usr/bin/env python3
"""
å¤„ç†å¤§å‹GitHubä»“åº“çš„è„šæœ¬
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

def main():
    print("=" * 70)
    print("å¤§å‹GitHubä»“åº“å¤„ç†å·¥å…·")
    print("=" * 70)
    
    # å¯¼å…¥æ™ºèƒ½çˆ¬å–å™¨
    try:
        from src.crawler.smart_crawler import SmartGitHubCrawler
        print("  æ™ºèƒ½çˆ¬å–å™¨å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"  å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·å…ˆåˆ›å»º smart_crawler.py æ–‡ä»¶")
        return
    
    # åˆå§‹åŒ–
    crawler = SmartGitHubCrawler(request_delay=0.5, max_issues_per_repo=100)
    
    if not crawler.is_connected():
        print("  GitHubæœªè¿æ¥")
        return
    
    # ç¤ºä¾‹ä»“åº“ï¼ˆå¯ä»¥æ›¿æ¢ä¸ºä½ çš„ç›®æ ‡ä»“åº“ï¼‰
    large_repo = "Shenzhizui/smart-code-qa-system"  # ä½ è‡ªå·±çš„ä»“åº“
    
    print(f"\n  ç›®æ ‡ä»“åº“: {large_repo}")
    print("-" * 50)
    
    # åˆ†æå¤§å‹ä»“åº“
    crawler.analyze_large_repository(large_repo)
    
    print(f"\n  å¼€å§‹æ™ºèƒ½æŠ½æ ·è·å–Issue...")
    
    # è·å–æ™ºèƒ½æ ·æœ¬
    sample_issues = crawler.get_issues_smart_sample(large_repo, sample_size=50)
    
    if sample_issues:
        print(f"\n  æˆåŠŸè·å– {len(sample_issues)} ä¸ªIssueæ ·æœ¬")
        
        # ä¿å­˜æ ·æœ¬æ•°æ®
        import json
        os.makedirs("data/large_repo", exist_ok=True)
        
        # è½¬æ¢ä¸ºå­—å…¸
        issues_data = []
        for issue in sample_issues:
            issue_dict = issue.to_dict()
            
            # é€‰æ‹©æ€§è·å–è¯„è®ºï¼ˆåªè·å–å‰3ä¸ªæœ‰è¯„è®ºçš„Issueï¼‰
            if issue.comments > 0 and len([i for i in issues_data if "comments_data" in i]) < 3:
                print(f"   è·å–Issue #{issue.number} çš„è¯„è®º...")
                comments = crawler.get_issue_comments(
                    large_repo, 
                    issue.number, 
                    max_comments=10
                )
                if comments:
                    issue_dict["comments_sample"] = [
                        {
                            "user": c.user,
                            "body_preview": c.body[:100] + "..." if len(c.body) > 100 else c.body,
                            "created_at": c.created_at
                        } for c in comments[:3]  # åªä¿å­˜å‰3æ¡è¯„è®º
                    ]
            
            issues_data.append(issue_dict)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = "data/large_repo/sample_issues.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(issues_data, f, ensure_ascii=False, indent=2)
        
        print(f"  æ ·æœ¬æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n  æ ·æœ¬ç»Ÿè®¡:")
        print(f"   æ€»Issueæ•°: {len(sample_issues)}")
        
        issues_with_comments = [i for i in sample_issues if i.comments > 0]
        print(f"   æœ‰è¯„è®ºçš„Issue: {len(issues_with_comments)}")
        
        avg_comments = sum(i.comments for i in sample_issues) / len(sample_issues)
        print(f"   å¹³å‡è¯„è®ºæ•°: {avg_comments:.1f}")
        
        # æ ‡ç­¾åˆ†å¸ƒ
        all_labels = []
        for issue in sample_issues:
            all_labels.extend(issue.labels)
        
        if all_labels:
            unique_labels = set(all_labels)
            print(f"   æ ‡ç­¾ç§ç±»: {len(unique_labels)}")
    
    print("\n" + "=" * 70)
    print("  å¤§å‹ä»“åº“å¤„ç†å®Œæˆ")
    print("=" * 70)
    
    print("\nğŸ’¡ åç»­å»ºè®®:")
    print("1. ä½¿ç”¨æ ·æœ¬æ•°æ®è¿›è¡Œå¼€å‘å’Œæµ‹è¯•")
    print("2. éœ€è¦å®Œæ•´æ•°æ®æ—¶å†åˆ†æ‰¹è·å–")
    print("3. é‡ç‚¹å…³æ³¨æœ‰è¯„è®ºå’Œé‡è¦æ ‡ç­¾çš„Issue")

if __name__ == "__main__":
    main()