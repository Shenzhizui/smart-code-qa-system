"""
æ•°æ®ç´¢å¼•å™¨ - å°†GitHubæ•°æ®è½¬æ¢ä¸ºå‘é‡å­˜å‚¨æ ¼å¼
"""
import os
import sys
import json
from typing import List, Dict, Any

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from .chroma_store import ChromaVectorStore
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    from src.vector_store.chroma_store import ChromaVectorStore


class DataIndexer:
    """æ•°æ®ç´¢å¼•å™¨ï¼Œè´Ÿè´£å°†GitHubæ•°æ®è½¬æ¢ä¸ºå‘é‡å­˜å‚¨æ ¼å¼"""
    
    def __init__(self, collection_name: str = "code_repository"):
        """
        åˆå§‹åŒ–ç´¢å¼•å™¨
        
        Args:
            collection_name: é›†åˆåç§°
        """
        self.vector_store = ChromaVectorStore(collection_name)
        print(f"âœ… æ•°æ®ç´¢å¼•å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   é›†åˆåç§°: {collection_name}")
    
    def index_code_files(self, code_files_data: List[Dict[str, Any]]):
        """
        ç´¢å¼•ä»£ç æ–‡ä»¶æ•°æ®
        
        Args:
            code_files_data: ä»£ç æ–‡ä»¶æ•°æ®åˆ—è¡¨
        """
        if not code_files_data:
            print("âš ï¸ æ²¡æœ‰ä»£ç æ–‡ä»¶æ•°æ®å¯ç´¢å¼•")
            return
        
        print(f"ğŸ“ å¼€å§‹ç´¢å¼• {len(code_files_data)} ä¸ªä»£ç æ–‡ä»¶...")
        
        documents = []
        for file_data in code_files_data:
            # æ„å»ºæ–‡æ¡£
            text = self._prepare_code_text(file_data)
            metadata = {
                "type": "code_file",
                "source": file_data.get("path", "unknown"),
                "language": file_data.get("language", "unknown"),
                "file_name": file_data.get("name", "unknown"),
                "repository": file_data.get("repo_name", "unknown"),
                "size": file_data.get("size", 0)
            }
            
            documents.append({
                "text": text,
                "metadata": metadata
            })
        
        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.vector_store.add_documents(documents)
        print(f"âœ… ä»£ç æ–‡ä»¶ç´¢å¼•å®Œæˆ: {len(documents)} ä¸ªæ–‡ä»¶")
    
    def index_issues(self, issues_data: List[Dict[str, Any]]):
        """
        ç´¢å¼•Issueæ•°æ®
        
        Args:
            issues_data: Issueæ•°æ®åˆ—è¡¨
        """
        if not issues_data:
            print("âš ï¸ æ²¡æœ‰Issueæ•°æ®å¯ç´¢å¼•")
            return
        
        print(f"ğŸ“ å¼€å§‹ç´¢å¼• {len(issues_data)} ä¸ªIssues...")
        
        documents = []
        for issue_data in issues_data:
            # æ„å»ºæ–‡æ¡£
            text = self._prepare_issue_text(issue_data)
            metadata = {
                "type": "issue",
                "source": issue_data.get("html_url", "unknown"),
                "issue_number": issue_data.get("number", 0),
                "state": issue_data.get("state", "unknown"),
                "creator": issue_data.get("user", {}).get("login", "unknown"),
                "repository": issue_data.get("repo_name", "unknown"),
                "created_at": issue_data.get("created_at", ""),
                "comments_count": issue_data.get("comments", 0)
            }
            
            # æ·»åŠ æ ‡ç­¾ä¿¡æ¯
            if issue_data.get("labels"):
                metadata["labels"] = json.dumps([label.get("name", "") for label in issue_data["labels"]])
            
            documents.append({
                "text": text,
                "metadata": metadata
            })
        
        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.vector_store.add_documents(documents)
        print(f"âœ… Issuesç´¢å¼•å®Œæˆ: {len(documents)} ä¸ªIssue")
    
    def index_pull_requests(self, prs_data: List[Dict[str, Any]]):
        """
        ç´¢å¼•Pull Requestæ•°æ®
        
        Args:
            prs_data: PRæ•°æ®åˆ—è¡¨
        """
        if not prs_data:
            print("âš ï¸ æ²¡æœ‰PRæ•°æ®å¯ç´¢å¼•")
            return
        
        print(f"ğŸ”€ å¼€å§‹ç´¢å¼• {len(prs_data)} ä¸ªPull Requests...")
        
        documents = []
        for pr_data in prs_data:
            # æ„å»ºæ–‡æ¡£
            text = self._prepare_pr_text(pr_data)
            metadata = {
                "type": "pull_request",
                "source": pr_data.get("html_url", "unknown"),
                "pr_number": pr_data.get("number", 0),
                "state": pr_data.get("state", "unknown"),
                "creator": pr_data.get("user", {}).get("login", "unknown"),
                "repository": pr_data.get("repo_name", "unknown"),
                "created_at": pr_data.get("created_at", ""),
                "merged": "true" if pr_data.get("merged", False) else "false",
                "comments_count": pr_data.get("comments", 0)
            }
            
            documents.append({
                "text": text,
                "metadata": metadata
            })
        
        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        self.vector_store.add_documents(documents)
        print(f"âœ… Pull Requestsç´¢å¼•å®Œæˆ: {len(documents)} ä¸ªPR")
    
    def index_readme_files(self, readme_data: List[Dict[str, Any]]):
        """
        ç´¢å¼•READMEæ–‡ä»¶æ•°æ®
        
        Args:
            readme_data: READMEæ•°æ®åˆ—è¡¨
        """
        if not readme_data:
            print("âš ï¸ æ²¡æœ‰READMEæ•°æ®å¯ç´¢å¼•")
            return
        
        print(f"ğŸ“– å¼€å§‹ç´¢å¼• {len(readme_data)} ä¸ªREADMEæ–‡ä»¶...")
        
        documents = []
        for readme in readme_data:
            text = self._prepare_readme_text(readme)
            metadata = {
                "type": "readme",
                "source": readme.get("path", "unknown"),
                "repository": readme.get("repo_name", "unknown"),
                "file_name": "README.md"
            }
            
            documents.append({
                "text": text,
                "metadata": metadata
            })
        
        self.vector_store.add_documents(documents)
        print(f"âœ… READMEæ–‡ä»¶ç´¢å¼•å®Œæˆ: {len(documents)} ä¸ªæ–‡ä»¶")
    
    def _prepare_code_text(self, file_data: Dict[str, Any]) -> str:
        """å‡†å¤‡ä»£ç æ–‡ä»¶æ–‡æœ¬"""
        content = file_data.get("content", "")
        language = file_data.get("language", "")
        path = file_data.get("path", "")
        
        # æ„å»ºæœ‰æ„ä¹‰çš„æ–‡æœ¬è¡¨ç¤º
        text = f"ä»£ç æ–‡ä»¶: {path}\n"
        text += f"ç¼–ç¨‹è¯­è¨€: {language}\n"
        if content:
            text += f"å†…å®¹:\n{content[:1000]}\n"  # é™åˆ¶å†…å®¹é•¿åº¦
        else:
            text += "å†…å®¹: [ç©º]\n"
        
        return text
    
    def _prepare_issue_text(self, issue_data: Dict[str, Any]) -> str:
        """å‡†å¤‡Issueæ–‡æœ¬"""
        title = issue_data.get("title", "")
        body = issue_data.get("body", "")
        labels = issue_data.get("labels", [])
        
        # æ„å»ºæœ‰æ„ä¹‰çš„æ–‡æœ¬è¡¨ç¤º
        text = f"Issue: {title}\n"
        
        # æ·»åŠ æ ‡ç­¾ä¿¡æ¯
        if labels:
            label_names = [label.get("name", "") for label in labels]
            text += f"æ ‡ç­¾: {', '.join(label_names)}\n"
        
        if body:
            text += f"æè¿°:\n{body[:2000]}\n"  # é™åˆ¶å†…å®¹é•¿åº¦
        else:
            text += "æè¿°: [ç©º]\n"
        
        return text
    
    def _prepare_pr_text(self, pr_data: Dict[str, Any]) -> str:
        """å‡†å¤‡Pull Requestæ–‡æœ¬"""
        title = pr_data.get("title", "")
        body = pr_data.get("body", "")
        
        # æ„å»ºæœ‰æ„ä¹‰çš„æ–‡æœ¬è¡¨ç¤º
        text = f"Pull Request: {title}\n"
        
        if body:
            text += f"æè¿°:\n{body[:2000]}\n"  # é™åˆ¶å†…å®¹é•¿åº¦
        else:
            text += "æè¿°: [ç©º]\n"
        
        return text
    
    def _prepare_readme_text(self, readme_data: Dict[str, Any]) -> str:
        """å‡†å¤‡READMEæ–‡æœ¬"""
        content = readme_data.get("content", "")
        path = readme_data.get("path", "")
        
        text = f"READMEæ–‡ä»¶: {path}\n"
        if content:
            text += f"å†…å®¹:\n{content[:3000]}\n"  # é™åˆ¶å†…å®¹é•¿åº¦
        else:
            text += "å†…å®¹: [ç©º]\n"
        
        return text
    
    def get_vector_store(self) -> ChromaVectorStore:
        """è·å–å‘é‡å­˜å‚¨å®ä¾‹"""
        return self.vector_store


# æµ‹è¯•å‡½æ•°
def test_data_indexer():
    """æµ‹è¯•æ•°æ®ç´¢å¼•å™¨"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®ç´¢å¼•å™¨")
    print("=" * 60)
    
    try:
        # åˆ›å»ºç´¢å¼•å™¨
        indexer = DataIndexer("test_indexer")
        
        # æ¨¡æ‹Ÿä»£ç æ–‡ä»¶æ•°æ®
        code_files = [
            {
                "path": "src/main.py",
                "content": "def main():\n    print('Hello World')",
                "language": "python",
                "name": "main.py",
                "repo_name": "test_repo",
                "size": 100
            }
        ]
        
        # æ¨¡æ‹ŸIssueæ•°æ®
        issues = [
            {
                "title": "æµ‹è¯•Issue",
                "body": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•Issue",
                "html_url": "https://github.com/test/repo/issues/1",
                "number": 1,
                "state": "open",
                "user": {"login": "test_user"},
                "repo_name": "test_repo",
                "created_at": "2024-01-01T00:00:00Z",
                "comments": 0,
                "labels": [{"name": "bug"}, {"name": "test"}]
            }
        ]
        
        # æ¨¡æ‹ŸPRæ•°æ®
        prs = [
            {
                "title": "æµ‹è¯•PR",
                "body": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•Pull Request",
                "html_url": "https://github.com/test/repo/pull/1",
                "number": 1,
                "state": "open",
                "user": {"login": "test_user"},
                "repo_name": "test_repo",
                "created_at": "2024-01-01T00:00:00Z",
                "merged": False,
                "comments": 0
            }
        ]
        
        # æµ‹è¯•ç´¢å¼•
        print("1. ç´¢å¼•ä»£ç æ–‡ä»¶...")
        indexer.index_code_files(code_files)
        
        print("\n2. ç´¢å¼•Issues...")
        indexer.index_issues(issues)
        
        print("\n3. ç´¢å¼•Pull Requests...")
        indexer.index_pull_requests(prs)
        
        # è·å–å‘é‡å­˜å‚¨
        vector_store = indexer.get_vector_store()
        info = vector_store.get_collection_info()
        
        print(f"\nğŸ“Š ç´¢å¼•å®Œæˆ:")
        print(f"   é›†åˆåç§°: {info['collection_name']}")
        print(f"   æ–‡æ¡£æ•°é‡: {info['document_count']}")
        
        # æµ‹è¯•æœç´¢
        print("\n4. æµ‹è¯•æœç´¢...")
        results = vector_store.search("æµ‹è¯•Issue", n_results=1)
        if results:
            print(f"âœ… æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°ç»“æœ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    test_data_indexer()